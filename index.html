<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks by imatge-upc</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-7678045-9', 'auto');
  ga('send', 'pageview');

</script>
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks</h1>
      <h2 class="project-tagline">1st NIPS Workshop on Large Scale Computer Vision Systems</h2>
      <a href="https://github.com/imatge-upc/activitynet-2016-cvprw" class="btn">View on GitHub</a>
      <a href="https://github.com/imatge-upc/activitynet-2016-cvprw/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/imatge-upc/activitynet-2016-cvprw/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">

<p>This project page describes our paper at the <a href="https://sites.google.com/site/largescalecvsystems/cfp/index">1st NIPS Workshop on Large Scale Computer Vision Systems</a>. This work also corresponds to the submission of the UPC team participating in the <a href="http://activity-net.org/challenges/2016/">ActivityNet Challenge</a> for CVPR 2016.</p>

<table>
<thead>
<tr>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/activitynet-2016-cvprw/master/misc/images/alberto_montes.jpg" alt="Alberto Montes" title="Alberto Montes"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/activitynet-2016-cvprw/master/misc/images/amaia_salvador.jpg" alt="Amaia Salvador" title="Amaia Salvador"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/activitynet-2016-cvprw/master/misc/images/xavier_giro.jpg" alt="Xavier Giró-i-Nieto" title="Xavier Giró-i-Nieto"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/activitynet-2016-cvprw/master/misc/images/santi_pascual.jpg" alt="Santiago Pascual" title="Santiago Pascual"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Main contributor</td>
<td align="center">Advisor</td>
<td align="center">Advisor</td>
<td align="center">Advisor</td>
</tr>
<tr>
<td align="center">Alberto Montes</td>
<td align="center"><a href="web-amaia">Amaia Salvador</a></td>
<td align="center"><a href="https://imatge.upc.edu/web/people/xavier-giro">Xavier Giró-i-Nieto</a></td>
<td align="center">Santiago Pascual</td>
</tr>
</tbody>
</table>

<p>Institution: <a href="http://www.upc.edu">Universitat Politècnica de Catalunya</a>.</p>

<p><img src="https://raw.githubusercontent.com/imatge-upc/activitynet-2016-cvprw/master/misc/images/upc_etsetb.jpg" alt="Universitat Politècnica de Catalunya"></p>

<h2>
<a id="abstract" class="anchor" href="#abstract" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Abstract</h2>

<p>Deep learning techniques have been proven to be a great success for tasks like object detection and classification.
They have achieve huge accuracy on images but on videos where the temporal dimension is present, more new techniques are required to face task over them.</p>

<p>Activity classification and temporal activity location require new models which try to explode the temporal correlations the videos present to achieve good results on this tasks. The work presented try to face this tasks, for both activity classification and temporal activity localization using the <a href="http://activity-net.org/download.html">ActivityNet Dataset</a>.</p>

<p>This work propose to face the tasks with a two stage pipeline. The first stage is to extract video features from the C3D which exploit temporal correlations and then a RNN made up by LSTM cells which try to learn long-term correlations and returning a sequence of activities along the video that will help to classify and temporally localize activities.</p>

<h2>
<a id="what-are-you-going-to-find-here" class="anchor" href="#what-are-you-going-to-find-here" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What Are You Going to Find Here</h2>

<p>This project is a baseline in the activity classification and its temporal location, focused on the <a href="http://activity-net.org/challenges/2016/">ActivityNet Challenge</a>. Here is detailed all the process of our proposed pipeline, as well the trained models and the utility to classify and temporally localize activities on new videos given. All the steps have been detailed, from downloading the dataset, to predicting the temporal locations going through the feature extraction and also the training.</p>

<h2>
<a id="publication" class="anchor" href="#publication" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Publication</h2>

  <p>Download our paper at the <a href="https://sites.google.com/site/largescalecvsystems/cfp/index">1st NIPS Workshop on Large Scale Computer Vision Systems</a> by clicking <a href="https://github.com/imatge-upc/activitynet-2016-cvprw/raw/master/temporal-activity-detection.pdf">here</a>. Please cite with the following Bibtex code:</p>

<pre><code>@InProceedings{Montes_2016_NIPSWS,
author = {Montes, Alberto and Salvador, Amaia and Pascual, Santiago and Giro-i-Nieto, Xavier},
title = {Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks},
booktitle = {1st NIPS Workshop on Large Scale Computer Vision Systems},
month = {December},
year = {2016}
}
</code></pre>
      
      <p>You may also want to refer to our publication with the more human-friendly Chicago style:</p>

<p><em>Alberto Montes, Amaia Salvador, Santiago Pascual, and Xavier Giro-i-Nieto. "Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks." In 1st NIPS Workshop on Large Scale Computer Vision Systems. 2016.</em></p>

      
<p>This work is the result of the <a href="http://arxiv.org/abs/1608.08128">bachelor thesis</a> by Alberto Montes at <a href="https://www.etsetb.upc.edu/en">UPC TelecomBCN ETSETB</a> during Spring 2016.</p>

<center>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/ki54iYd4YFV1U8" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/xavigiro/temporal-activity-detection-in-untrimmed-videos-with-recurrent-neural-networks" title="Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks" target="_blank">Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks</a> </strong> from <strong><a href="//www.slideshare.net/xavigiro" target="_blank">Xavier Giro</a></strong> </div>
</center>
<br>
<center>
<iframe width="420" height="315" src="https://www.youtube.com/embed/3G-Vdmsluw0" frameborder="0" allowfullscreen></iframe>
</center>

<h2>
<a id="demo" class="anchor" href="#demo" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demo</h2>

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/WGjzicaVbBU" frameborder="0" allowfullscreen></iframe>  
</center>

<h2>
<a id="repository-structure" class="anchor" href="#repository-structure" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Repository Structure</h2>

<p>This repository is structured in the following way:</p>

<ul>
<li>
<code>data/</code>: dir where, by default, all the data such as videos or model weights are stored. Some data is given such ass the C3D means and also provide scripts to download the weights for the C3D model and the one we propose.</li>
<li>
<code>dataset/</code>: files describing the ActivityNet dataset and a script to download all the videos. The information of the dataset has been extended with the number of frames at each of the videos.</li>
<li>
<code>misc/</code>: directory with some miscellaneous information such as all the details of the steps followed on this project and much more.</li>
<li>
<code>notebooks/</code>: notebooks with some visualization of the results.</li>
<li>
<code>scripts/</code>: scripts to reproduce all the steps of project.</li>
<li>
<code>src/</code>: source code required for the scripts.</li>
</ul>

<h2>
<a id="dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h2>

<p>This project is build using the <a href="https://github.com/fchollet/keras">Keras</a> library for Deep Learning, which can use as a backend both <a href="https://github.com/Theano/Theano">Theano</a>
and <a href="https://github.com/tensorflow/tensorflow">TensorFlow</a>.</p>

<p>We have used Theano in order to develop the project because it supported 3D convolutions and pooling required to run the C3D network.</p>

<p>For a further and more complete of all the dependencies used within this project, check out the requirements.txt provided within the project. This file will help you to recreate the exact same Python environment that we worked with.</p>

<h2>
<a id="pipeline" class="anchor" href="#pipeline" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline</h2>

<p>The pipeline proposed to face the ActivityNet Challenge is made up by two stages.</p>

<p>The first stage encode the video information into a single vector representation for small video clips. To achieve that, the C3D network [Tran2014] is used. The C3D network uses 3D convolutions to extract spatiotemporal features from the videos, which previously have been split in 16-frames clips.</p>

<p>The second stage, once the video features are extracted, is to classify the activity on each clip as the videos of the ActivityNet are untrimmed and may be an activity or not (background). To perform this classification a RNN is used. More specifically a LSTM network which tries to exploit long term correlations and perform a prediction of the video sequence. This stage is the one which has been trained.</p>

<p>The structure of the network can be seen on the next figure.</p>

<p><img src="https://raw.githubusercontent.com/imatge-upc/activitynet-2016-cvprw/master/misc/images/network_pipeline.jpg" alt="Network Pipeline"></p>

<p>To reproduce all the process of the pipeline, there is a <a href="https://github.com/imatge-upc/activitynet-2016-cvprw/blob/master/misc/step_by_step_guide.md">detailed guide</a> about how to reproduce all the steps with the scripts provided.</p>

<h2>
<a id="related-work" class="anchor" href="#related-work" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Related work</h2>

<ul>
<li>Tran, D., Bourdev, L., Fergus, R., Torresani, L., &amp; Paluri, M. (2015, December). Learning spatiotemporal features with 3d convolutional networks. In 2015 IEEE International Conference on Computer Vision (ICCV) (pp. 4489-4497). IEEE. [<a href="http://arxiv.org/pdf/1412.0767.pdf">paper</a>] [<a href="https://github.com/facebook/C3D">code</a>]</li>
<li>Sharma, S., Kiros, R., &amp; Salakhutdinov, R. (2015). Action recognition using visual attention. arXiv preprint arXiv:1511.04119. [<a href="http://arxiv.org/pdf/1511.04119.pdf">paper</a>][<a href="https://github.com/kracwarlock/action-recognition-visual-attention">code</a>]</li>
<li>Yeung, S., Russakovsky, O., Mori, G., &amp; Fei-Fei, L. (2015). End-to-end Learning of Action Detection from Frame Glimpses in Videos. arXiv preprint arXiv:1511.06984. [<a href="http://arxiv.org/pdf/1511.06984.pdf">paper</a>]</li>
<li>Yeung, Serena, et al. "Every moment counts: Dense detailed labeling of actions in complex videos." arXiv preprint arXiv:1507.05738 (2015).[<a href="http://arxiv.org/pdf/1507.05738v2.pdf">paper</a>]</li>
<li>Baccouche, M., Mamalet, F., Wolf, C., Garcia, C., &amp; Baskurt, A. (2011, November). Sequential deep learning for human action recognition. In International Workshop on Human Behavior Understanding (pp. 29-39). Springer Berlin Heidelberg. [<a href="https://www.researchgate.net/profile/Moez_Baccouche/publication/221620711_Sequential_Deep_Learning_for_Human_Action_Recognition/links/53eca3470cf250c8947cd686.pdf">paper</a>]</li>
<li>Shou, Zheng, Dongang Wang, and Shih-Fu Chang. "Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs." [<a href="http://dvmmweb.cs.columbia.edu/files/dvmm_scnn_paper.pdf">paper</a>] [<a href="https://github.com/zhengshou/scnn">code</a>]</li>
</ul>

<h2>
<a id="acknowledgements" class="anchor" href="#acknowledgements" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Acknowledgements</h2>

<p>We would like to especially thank Albert Gil Moreno and Josep Pujal from our technical support team at the Image Processing Group at the UPC.</p>

<table>
<thead>
<tr>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/activitynet-2016-cvprw/master/misc/images/albert_gil.jpg" alt="Albert Gil" title="Albert Gil"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/activitynet-2016-cvprw/master/misc/images/josep_pujal.jpg" alt="Josep Pujal" title="Josep Pujal"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="web-albert">Albert Gil</a></td>
<td align="center"><a href="web-josep">Josep Pujal</a></td>
</tr>
</tbody>
</table>

<h2>
<a id="contact" class="anchor" href="#contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h2>

<p>If you have any general doubt about our work or code which may be of interest for other researchers, please use the <a href="https://github.com/imatge-upc/activitynet-2016-cvprw/issues">issues section</a>
on this github repo. Alternatively, drop us an e-mail at <a href="mailto:xavier.giro@upc.edu">xavier.giro@upc.edu</a>.</p>





      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/imatge-upc/activitynet-2016-cvprw">Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks</a> is maintained by <a href="https://github.com/imatge-upc">imatge-upc</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
